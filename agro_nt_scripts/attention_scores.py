import os
import torch
import argparse
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig

def extract_attention(model_path, sequences, output_dir, max_seq_length=512, cache_dir=None):
    # Load configuration and set output_attentions=True
    config_path = os.path.join(model_path, "config.json")
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"config.json not found in {model_path}")
    
    print(f"Loading model configuration from {config_path}...")
    config = AutoConfig.from_pretrained(config_path)
    config.output_attentions = True  # Ensure attentions are returned
    config.num_labels = 1

    # Load the model with the updated config
    print(f"Loading model from {model_path}...")
    model = AutoModelForSequenceClassification.from_pretrained(
        model_path,
        config=config,
        cache_dir=cache_dir,
    )

    # Load the tokenizer
    print(f"Loading tokenizer from {model_path}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)

    # Ensure the model is in evaluation mode
    model.eval()

    # Tokenize input sequences
    inputs = tokenizer(
        sequences,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_seq_length,
    )
    print(f"Tokenized input IDs: {inputs['input_ids']}")
    print(f"Number of tokens: {inputs['input_ids'].shape[1]}")
    print(f"Special tokens used: {tokenizer.special_tokens_map}")
    print(f"Decoded tokens: {tokenizer.batch_decode(inputs['input_ids'])}")

    # Save token-to-sequence mapping
    token_id_to_sequence = {}
    for idx, token_id in enumerate(inputs['input_ids'][0].tolist()):
        token_id_to_sequence[idx] = tokenizer.decode([token_id])

    token_map_path = os.path.join(output_dir, "token_id_to_sequence.json")
    with open(token_map_path, "w") as f:
        import json
        json.dump(token_id_to_sequence, f)
    print(f"Token-to-sequence mapping saved to {token_map_path}")

    # Pass inputs to the model
    with torch.no_grad():
        outputs = model(**inputs)

    # Ensure attentions are returned
    if outputs.attentions is None:
        raise ValueError(
            "The model did not return attention scores. "
            "Ensure that the model is configured to output attentions by setting `output_attentions=True` in its configuration."
        )

    # Average attention scores per layer
    avg_attention_per_layer = [
        torch.mean(layer_attention, dim=(0, 1))  # Average over batch and heads
        for layer_attention in outputs.attentions
    ]

    # Save attention scores
    for i, attention in enumerate(avg_attention_per_layer):
        torch.save(attention, os.path.join(output_dir, f"layer_{i}_attention.pt"))

    print(f"Attention scores saved in {output_dir}")


# Run the script
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extract attention scores from a model")
    parser.add_argument("--model_path", type=str, required=True, help="Path to the model checkpoint")
    parser.add_argument("--output_dir", type=str, required=True, help="Directory to save attention scores")
    parser.add_argument("--max_seq_length", type=int, default=512, help="Maximum sequence length for the tokenizer")
    parser.add_argument("--cache_dir", type=str, required=True, help="Directory for caching models and tokenizers")
    
    args = parser.parse_args()

    # Note the length of this sequence is 2000 - which makes sense from the plot!
    #sequences = ["TTTATATCACAATCACTAGTTTATTTTATATTAATAATTATTTTTAAAAAAATAAATGTATCAAATTAATAGGCCTTTATTATAAAGGCTTGTTTGGGTAAATAGGATTAAGGACTTAATAATTAAATAAGTGTTCATCATAAAAGTACTTATGCATGGATTGTTTATATAATCGAATAGAAAATAAGATTATTGAAATTGAGTCGAAAATAGATAGATCTATTTATTTTCCTCTATATATAAATTAAACCAAGTTTTTTAAATATTTTTATGTGTGTGCTTATGTAATAAAATATATTTAAATAAATTATAAATATATTTTTCCAATCGCATCCTAAATATTGGATATTCTAAATTTTAGTGAAAAAAGTTATTTGTTATATATGTACTAAAATTGCTTTCAAATTTAAAAAGCAATCTTTTAGTCCTCAAATTTAGAGATAACTCATTTTGGTTCTTCAAGTTGATGTTAAAAAATTTGTTGGCGTGAATAATAATGTATTTACTTACAATTTTTAGAAATACGATTTATGAAAATTTTAAACTTTCAAAATTTAGATTAACCAATAATGTTTTTAAAAATATGTTTTGTGGAGGTTTTAAACTCAAAAATCCAGTAAACACAAAAGAAAAAAAATGACAATGCAACAACAATCTTTCAATGTCCAACTCAACTTCTCAATCTTCAATCTTTCTTCTTTCTTTAATTTCAGCCTCTACATCTTTCCTACATCAATAACATTTGTCAACTTCTTTGTTTGCCTCTTCCATTTCTATTCTCCAATTTCACCCTCACCTTTTCTCATGCTCCCTCCTCTCTCTTGCTCTTCCCTTCTCACTTCTCCCAGCCTTGATGACCACTACCACCATTATGACAACATGGTCCCATCTCATCTATCTTATTCTTACATTTGCTTGCTCATTCTCTCTTTAACTCCTGCTCATCCTCGCTCACCCTCCCATATAAAATGGGGTAGATTACATGTTTCAACTTTCAATCTACATCTCTTAAACGGGTTCACTTAGTCCGATTCACTTTTGCCAAGATGATAGAGGGCATGCCAAAATGTGCCATATAATCTGTTACCACCACTAATCGCAAGTAAGTTATGGTGCATGTATAGGATTCGGATAAAGTATTTTATGATAAATCTAGTTAAAATGTTACAAGAGGCTTAAAAGATTAGTATTTTGTTGCTTAGTCATAAATTGAGATTTATTAGTACTATATAAGTTTGTTAAAATAATTTTCCTTAAAAATTAAATTTTATAATAATAATTTCAAAAGTATCCTGAAATTGTTTTAATGTAATTTACATTACATACTTTATTAAAAACAATGTTTGGCATATTTTTTATAATTAAAATGGAAATATTTTTAAAGAATTGTTTTGTTAGTCCCTAAAATTTGAGAATTTTTAAATAAGTTTCTAAACTATAAAAATTAATTTATCCTTAAACTGAAAAAAGTATGTCCTTGTTTGCTTTTGGTGATCAGAAAGTATGTCCTTATAATTTGTTTATTTCGACTTGAAATTGAAAGAAAAAAGGACAATTTATAACGAGTTTAAAATCTTATAAACCAAATTAAAAAAAAAAATCATGATCAATAACAGTCTTAAAAAAAGAAGTTTGAAGACCTATTTAGAAAATTTCAAATCGTTTAGGGCCAGTTGAGTAGTTTAACGATTTTTGAATTACACCTGAACACCAGGTACGTAAAAAAATAAAACAAAACAAAAACACTTCCTCTTCCTCCTTTCCCCCGAAAAACCCTTAAACCACGACCTTCATATTTGCAGTGCTAGTCTGTTTATTTCTTCAGATGTTTTTGTCAAGAACACTTGGACGAACACTCTTTGCTGCTGCTGCCAGATCAAAACAATATGCCACTACAGCAGCTGCTGCTAGCAGAGAAGGACACAACCCCCTTCAGGAGTTTTTTGAGGCAGACAGGAGCCCTGAAGATGACAAACCTGTTGTTTATGGTCGGAGTTGGAAG"]

    sequences = ['ATTAATTCATAAGTAAATGACATATGATAAGATTATTATTTTTGTAATAATAATTAATTTAAAACTTAATATTTATAGTGATTTCTAATTAATAATTAAATACAGAATAAAATTAAATAAAATAATAATAAATTGATAGTTCATAGTTATAATAATAGTAATAATAAATGCAAGGACGTATGTTACCTCCAATCCAACGCTGTTGAGGACTTGAAGCGGGTTGGACCACCAAACACTCTTATCATGGGCCCATGCCGCGTATGAATAATAAATGCACCAACTTCACACCGAATACTAATTCTAAAAGAAGATAATAAAAATATAGTATAATTTTTATTTTTTACTATTTTTTATTTTAAAACAATAATTATTTCGTCACTAGTTATTTGATACAAAATACCTTTTTTCTGCTACAAACTTACTATGATAACATAAAATCCTTCGCAAAACAATTTAATTAAATAAGTTGAATAAAATTTCAAGAACAACATTAAAGGTAATTGATAAATTTTATCGAAGACAACTTACATCCTTTATAAGAAATAATTATATTAAAATAGAAAAAAAAAATATCTACAACATATTTGAGTTAATTCTTGCAGCAAGAATTTTTCTCTATATATACAATAGTTGAACTGAAATTTTATTTTGAAAAAATGAATATACACACATATATATATATATATATATATATATACAAATAATTGTCATCCTCAACACTCAATATGAGTTTAAAACTACTGAACCTAATCCATTATTCATTTTTTAAAATCTCCCTAATTTAAAATAATCAAACACCAACGTAAGAAAGTTTTTTTTTTTTTTTTAATGCAAAACGTAAGGAAGTTATTCACAAACGTTAAAGACGGTTCATAACAACTATTGTTTTCCTTTTTCACTTATTTTTTTCCACATCTATCATTTTTGTCTATGAACAAATCAAAACCAATCACGTGACTTGTAAATGTTTATTAATACTTAAGACATTCCTTTAAATTTAATCTTTGTTTAAAAGTGATCTAAATGCACCTATTATAAAGTTATAATAATAAAACAAAATTTATATTTCAATTTTTTAAAATTTATAAAAAAAATATAAAATGTGTAATGAAAGGCACTTATGTGTTAATAAATACATACGAGTCATGTGATGTACGAGTTTGCTATAACTTATCATTAAAAGAGCCAGTTATAAATTTCAGAAAAAAACTTCTTCCAATTTGGAAATATGTACTACTTCAAAAGGAAGTAAAATTGCGATAGTGTATATATATGTTAGTGTTCGTTTTTCTTAAAATTTCATACATTTAAATCGTAATATATAACTAGAATAATTTGTATTCAAACTAATCTTAATTAATCTGCATTATAGTGTAGAAAAAGTATTTTCGCATGTTATTATCCAATTAAATACCCAGTTCCATTTTTTAATGCAGAGTTGGAAACTATTGCATTTGCAAAGACTAATATTAAAAAGGAAAAAAATAATTGGCCGGTACAAAAAAGAGGGAATGAGTGAGAATCATGTGAAGTGCCGCCActctctctctctttctctctctatctgtctctGGTTCAATCAAAAAAAAGCTTCAATCTTTAACAATCCAACACACAACAAAACAACCCCACTTCTGTTTTGTTTTGCTTAAATTATTCTGAGAAAGTTTCTTCTTTTCTTCTTGTTCCAATGGCTGCTGCAACTGCCCCAAGATTCATCAAGTGTGTGACCGTTGGCGATGGAGCTGTAGGGAAGACCTGCATGCTCATTTGCTATACCAGCAACAAATTCCCCACGGACTATATCCCCACTGTGTTTGATAATTTCAGTGCAAATGTGGTTGTTGAAGGCATAACTGTCAATTTAGGCCTTTGGGATACAGCTGGGCAAGAGGATTACAACAGGCTGAGGCCCTTGAGCTACAGGGGGGCAGATGTCTTTGTCTTGGCTTTTTCTTTAGTTAGTCGCGCAAGCTATGAGAATGTGCTGAAGAAGTGGATCCCTGAACTCCAGCATTTTGCCCCTGGCATCCCATTGGTATTAGTTGGCACAAAATTGGATCTGCGCGAAGACAGGCACTATATGGCTGATCATCCTGGCTTGGTGCCCGTGACTACTGAGCAAGGTGAGGAACTCCGTAAACATATTGGAGCTACCTACTATATTGAGTGCAGCTCAAAAACTCAGCAGAATGTGAAGGCAGTTTTTGATGCTGCGATCAGAATGGTCATCAAGCCTCCACAAAAGCAAAACGAGAAAAGGAAGAAAAAACCACGTGGCTGTTTCCTAAATGTCCTCTGTGGAAGGAACATTGTTCGTCTTAAGTGAAAACACTTCATCAGTATCTCATCCAAAGGTTTCACGTGCTAGGGGGAGAAGCCTGTTACCGCCGTGTAGAAATTTTAAACTTAACCTAAAATCACTTGGCTGGAATTTGCAGAATAGCTACTTACATTTGGTTATATTTAATTTAATTTAATATCTTATGGGTTTTAATGTATTGAGGAACTATAAAAAACAGTTGCACCCGATAGCTTGTAAATAAAGCAGAGTGAGTTTGTTTGCTAGCTACCCTTTATTTATTTTGGAAGCGTTCAAGTT']
    
    extract_attention(
        model_path=args.model_path,
        sequences=sequences,
        output_dir=args.output_dir,
        max_seq_length=args.max_seq_length,
        cache_dir=args.cache_dir,
    )
